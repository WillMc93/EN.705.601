{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Describe the environment in the Nim learning model.\n",
    "\n",
    "The environment of the learning model is the 'board' which consists of a list of stacks that contain some quantity of objects (3 stacks of at most 10 items in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Describe the agent in the Nim learning model.\n",
    "\n",
    "Our agent is a Q-Learning agent. This agent uses a table (*qtable*) of expected payoffs to make \"educated\" guesses about which stack to pull objects out of based on the highest expected reward based on the current state of the environment. The number of items pulled (*move*) is equal to a modulo of the index of the highest reward (*a*) by the maximum number of states of a stack, The stack pulled from (*pile*) is determined by an integer division of the index of the highest reward (*a*) by the maximum number of items in a stack.\n",
    "\n",
    "If this procedure produces an illegal move then the agent will instead make a random legal move. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Describe the reward and penalty in the Nim learning model.\n",
    "\n",
    "In this model, reward/penalty is determined by the function $\\alpha * (reward + \\gamma * q'_{best})$. In this equation, $\\alpha$ adjust the gain for the learning function, $reward$ is gives a large boost to moves that result in a win, and the $\\gamma *q'_{best}$ term assures that all legal moves get at least some reward (but less than a win). The $q'_{best}$ term also means that the model won't be able to learn moves until after a game has been won, as that is the only way to set any of the terms in *qtable* above zero (*qtable* is zeroed out at the beginning of the learning function).\n",
    "\n",
    "Actions of the default qlearner:\n",
    "\n",
    "* Win: $reward = 100$; $q'_{best}$ set to 0. Sets a move as the correct one for a win.\n",
    "* Legal Move: $reward = 0$. Makes this move a fraction ($\\gamma$) of the best move for that state.\n",
    "* Loss: No special action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?\n",
    "\n",
    "The number of states is solvable as the product of the possible states of the stacks. The possible states of the stacks is just equal to the max occupancy plus one (for the empty state).\n",
    "\n",
    "So in this case the possible number of states for 3 stacks ($st_1$, $st_2$, $st_3$) of at max $10$ items each is equal to $(st_1 + 1)(st_2 + 1)(st_2 + 1) = 11^3 = 1331$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How many possible actions there could be in the Nim game with 10 items per pile and 3 piles total?\n",
    "\n",
    "The only action an agent may take is to take objects from a stack. The number of possible actions for each stack is then just the number of items in that stack. Therefore the total number of actions an agent may take is equal to the sum of the stack occupancies. In this case with 3 stacks of 10 items, we have $st_1 + st_2 + st_3 = 3 * 10 = 30$ possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Find a way to improve the provided Nim game learning model\n",
    "\n",
    "Code taken/adapted from module 9 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "from os import mkdir, path\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Accomadate and organize multiple logs\n",
    "log_path = './logs/'\n",
    "if not path.isdir(log_path):\n",
    "    mkdir(log_path)\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>13s} vs. {b:>13s}, {a} Winrate: {100 * wins['A'] / (wins['A'] + wins['B']):.2f}%\")\n",
    "    #\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn, qtable):\n",
    "    fn = log_path + _fn\n",
    "    \n",
    "    with open(fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        #\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            #\n",
    "            print(s, file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default QLearner\n",
    "\n",
    "Modified from module 9 notebook for modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha, Gamma, Reward = 1.0, 0.8, 100.0\n",
    "\n",
    "def nim_qlearner(_st, qtable, explore):\n",
    "   \n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = (a%ITEMS_MX)+1, a//ITEMS_MX\n",
    "    \n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = explore(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n, qtable, explore):\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = explore(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable = qtable_update(qtable, Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable = qtable_update(qtable, 0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "            \n",
    "    return qtable\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(qtable, r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)\n",
    "    return qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 759 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train and store\n",
    "default_qtable = None\n",
    "default_qtable = nim_qlearn(10000, qtable=default_qtable, explore=nim_random)\n",
    "default_qlearner = partial(nim_qlearner, qtable=default_qtable, explore=nim_random)\n",
    "\n",
    "# add to the Engines dictionary\n",
    "Engines['DQlearner'] = default_qlearner\n",
    "\n",
    "qtable_log('default_qtable.csv', default_qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guru QLearner\n",
    "\n",
    "Uses the *nim_guru()* function for exploration instead of just random (there will still be some randomness initially). This makes a little more sense than leaving things completely up to chance as we know this game is mathematically solved.\n",
    "\n",
    "This performs better than the default Qlearner, but if *nim_guru()* is allowed to go first this agent will not win the majority (vice-versa if this agent is allowed the first move versus *nim_guru()* it *will* win the majority)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 673 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train and store\n",
    "guru_qtable = None\n",
    "guru_qtable = nim_qlearn(10000, qtable=guru_qtable, explore=nim_guru)\n",
    "guru_qlearner = partial(nim_qlearner, qtable=guru_qtable, explore=nim_guru)\n",
    "\n",
    "# add to the Engines dictionary\n",
    "Engines['guru_Qlearner'] = guru_qlearner\n",
    "\n",
    "qtable_log('guru_qtable.csv', guru_qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossy Q-Learner\n",
    "\n",
    "Not a winner. I mean it wins games, but it's no better than *guru_QLeaner* or *nim_guru*. Still trains better if we use *nim_guru* as the exploration function, which is to be expected. *nim_random* does produce okay results.\n",
    "\n",
    "This could definitely be tuned a little.\n",
    "\n",
    "Code is adapted from the module 9 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn from _n games, randomly played to explore the possible states\n",
    "def lossy_qlearn(_n, qtable, explore):\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        st0 = None\n",
    "        prev_move, prev_pile = None, None\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = explore(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable = qtable_update(qtable, Reward, st1, move, pile, 0)  # I won\n",
    "                qtable = qtable_update(qtable, 0.5*np.max(qtable[st1]), st0, prev_move, prev_pile, 0) # I lost\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable = qtable_update(qtable, 0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            \n",
    "            st0 = st1\n",
    "            st1 = st2\n",
    "            prev_move = move\n",
    "            prev_pile = pile\n",
    "            \n",
    "    return qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 813 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train and store\n",
    "lossy_qtable = None\n",
    "lossy_qtable = lossy_qlearn(10000, qtable=lossy_qtable, explore=nim_guru)\n",
    "lossy_qlearner = partial(nim_qlearner, qtable=lossy_qtable, explore=nim_guru)\n",
    "\n",
    "# add to the Engines dictionary\n",
    "Engines['lossy_Qlearner'] = lossy_qlearner\n",
    "\n",
    "qtable_log('lossy_qtable.csv', lossy_qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIGHT!\n",
    "\n",
    "Pit agents against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,        Random vs.        Random, Random Winrate: 51.50%\n",
      "1000 games,        Random vs.          Guru, Random Winrate: 0.50%\n",
      "1000 games,        Random vs.     DQlearner, Random Winrate: 30.40%\n",
      "1000 games,        Random vs. guru_Qlearner, Random Winrate: 0.70%\n",
      "1000 games,        Random vs. lossy_Qlearner, Random Winrate: 1.20%\n",
      "\n",
      "\n",
      "1000 games,          Guru vs.        Random, Guru Winrate: 99.90%\n",
      "1000 games,          Guru vs.          Guru, Guru Winrate: 94.70%\n",
      "1000 games,          Guru vs.     DQlearner, Guru Winrate: 100.00%\n",
      "1000 games,          Guru vs. guru_Qlearner, Guru Winrate: 95.20%\n",
      "1000 games,          Guru vs. lossy_Qlearner, Guru Winrate: 94.00%\n",
      "\n",
      "\n",
      "1000 games,     DQlearner vs.        Random, DQlearner Winrate: 73.60%\n",
      "1000 games,     DQlearner vs.          Guru, DQlearner Winrate: 2.70%\n",
      "1000 games,     DQlearner vs.     DQlearner, DQlearner Winrate: 87.30%\n",
      "1000 games,     DQlearner vs. guru_Qlearner, DQlearner Winrate: 7.70%\n",
      "1000 games,     DQlearner vs. lossy_Qlearner, DQlearner Winrate: 10.30%\n",
      "\n",
      "\n",
      "1000 games, guru_Qlearner vs.        Random, guru_Qlearner Winrate: 99.90%\n",
      "1000 games, guru_Qlearner vs.          Guru, guru_Qlearner Winrate: 95.00%\n",
      "1000 games, guru_Qlearner vs.     DQlearner, guru_Qlearner Winrate: 99.70%\n",
      "1000 games, guru_Qlearner vs. guru_Qlearner, guru_Qlearner Winrate: 93.90%\n",
      "1000 games, guru_Qlearner vs. lossy_Qlearner, guru_Qlearner Winrate: 93.00%\n",
      "\n",
      "\n",
      "1000 games, lossy_Qlearner vs.        Random, lossy_Qlearner Winrate: 99.70%\n",
      "1000 games, lossy_Qlearner vs.          Guru, lossy_Qlearner Winrate: 93.50%\n",
      "1000 games, lossy_Qlearner vs.     DQlearner, lossy_Qlearner Winrate: 100.00%\n",
      "1000 games, lossy_Qlearner vs. guru_Qlearner, lossy_Qlearner Winrate: 94.30%\n",
      "1000 games, lossy_Qlearner vs. lossy_Qlearner, lossy_Qlearner Winrate: 94.80%\n"
     ]
    }
   ],
   "source": [
    "# LEEET'S GET READY TO RUUUUUMBLLLLLE!\n",
    "prev_a = None\n",
    "for a, b in product(Engines, repeat=2):\n",
    "    if not prev_a:\n",
    "        prev_a = a\n",
    "    elif prev_a != a:\n",
    "        # breaks our fights up by who goes first\n",
    "        prev_a = a\n",
    "        print('\\n')\n",
    "    \n",
    "    play_games(1000, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
