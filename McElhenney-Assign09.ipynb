{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Describe the environment in the Nim learning model.\n",
    "\n",
    "The environment of the learning model is the 'board' which consists of a list of stacks that contain some quantity of objects (3 stacks of at most 10 items in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Describe the agent in the Nim learning model.\n",
    "\n",
    "Our agent is a Q-Learning agent. This agent uses a table (*qtable*) of expected payoffs to make \"educated\" guesses about which stack to pull objects out of based on the highest expected reward (*a*) based on the current state of the environment. The number of items pulled (*move*) is equal to a modulo of the expected payoff by the maximum number of states of a stack. The stack pulled from (*pile*) is determined by an integer division of the payoff (*a*) by the maximum number of items in a stack.\n",
    "\n",
    "To be honest I'm a little confused as to why/how the stack selection works as it seems that it is more or less random which stack it tries to pull objects from (based on reward function not truly random, but still).\n",
    "\n",
    "If this procedure produces an illegal move then the agent will instead make a random legal move. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Describe the reward and penalty in the Nim learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How many possible states there could be in the Nim game with a maximum of 10 items per pile and 3 piles total?\n",
    "\n",
    "The number of states is solvable as the product of the possible states of the stacks. The possible states of the stacks is just equal to the max occupancy plus one (for the empty state).\n",
    "\n",
    "So in this case the possible number of states for 3 stacks ($st_1$, $st_2$, $st_3$) of at max $10$ items each is equal to $(st_1 + 1)(st_2 + 1)(st_2 + 1) = 11^3 = 1331$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. How many possible actions there could be in the Nim game with 10 items per pile and 3 piles total?\n",
    "\n",
    "The only action an agent may take is to take objects from a stack. The number of possible actions for each stack is then just the number of items in that stack. Therefore the total number of actions an agent may take is equal to the sum of the stack occupancies. In this case with 3 stacks of 10 items, we have $st_1 + st_2 + st_3 = 3 * 10 = 30$ possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Find a way to improve the provided Nim game learning model\n",
    "\n",
    "Code taken from module 9 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    #\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_nim_qlearner(_st):\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    \n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    #\n",
    "    return move, pile  # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'DQlearner':default_nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        #\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s} {wins['A']}   {b:>8s} {wins['B']}\")\n",
    "    #\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n",
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=float)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True:  # while game not finished\n",
    "            # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move  # --> last move I made\n",
    "            if st2 == [0, 0, 0]:  # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0)  # I won\n",
    "                break  # new game\n",
    "            #\n",
    "            qtable_update(0, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "            st1 = st2\n",
    "\n",
    "# Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.96 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nim_qlearn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, DQlearner 586     Random 414\n",
      "1000 games, DQlearner 592     Random 408\n",
      "1000 games, DQlearner 649     Random 351\n",
      "1000 games, DQlearner 701     Random 299\n",
      "1000 games, DQlearner 730     Random 270\n",
      "1000 games, DQlearner 716     Random 284\n",
      "1000 games, DQlearner 696     Random 304\n",
      "Wall time: 8.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'DQlearner', 'Random')\n",
    "    wins += [a/(a+b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        #\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            #\n",
    "            print(s, file=fout)\n",
    "#\n",
    "qtable_log('qtable_debug.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-8-2ed3c789538a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-2ed3c789538a>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def my_nim_qlearner(_st):\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def my_nim_qlearner(_st):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
