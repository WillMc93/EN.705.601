{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.) Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\whm0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\whm0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\whm0004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import socket\n",
    "\n",
    "from itertools import product\n",
    "from os.path import isfile\n",
    "from random import randint\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.neural_network import MLPClassifier as nn\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "# Download the texts\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sns.set(style='ticks', color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.)  Find interesting phrases in Alice In Wonderland.\n",
    "\n",
    "Most code adapted from module 10 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['alice', 'adventures', 'wonderland', 'lewis', 'carroll'],\n",
       " ['down', 'rabbit', 'hole'],\n",
       " ['alice',\n",
       "  'beginning',\n",
       "  'get',\n",
       "  'tired',\n",
       "  'sitting',\n",
       "  'sister',\n",
       "  'bank',\n",
       "  'nothing',\n",
       "  'twice',\n",
       "  'peeped',\n",
       "  'book',\n",
       "  'sister',\n",
       "  'reading',\n",
       "  'pictures',\n",
       "  'conversations',\n",
       "  'use',\n",
       "  'book',\n",
       "  'thought',\n",
       "  'alice',\n",
       "  'without',\n",
       "  'pictures',\n",
       "  'conversation'],\n",
       " ['considering',\n",
       "  'mind',\n",
       "  'well',\n",
       "  'could',\n",
       "  'hot',\n",
       "  'day',\n",
       "  'made',\n",
       "  'feel',\n",
       "  'sleepy',\n",
       "  'stupid',\n",
       "  'whether',\n",
       "  'pleasure',\n",
       "  'making',\n",
       "  'daisy',\n",
       "  'chain',\n",
       "  'would',\n",
       "  'worth',\n",
       "  'trouble',\n",
       "  'getting',\n",
       "  'picking',\n",
       "  'daisies',\n",
       "  'suddenly',\n",
       "  'white',\n",
       "  'rabbit',\n",
       "  'pink',\n",
       "  'eyes',\n",
       "  'ran',\n",
       "  'close'],\n",
       " ['there',\n",
       "  'nothing',\n",
       "  'very',\n",
       "  'remarkable',\n",
       "  'alice',\n",
       "  'think',\n",
       "  'very',\n",
       "  'much',\n",
       "  'way',\n",
       "  'hear',\n",
       "  'rabbit',\n",
       "  'say',\n",
       "  'dear']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the text\n",
    "sentences = gutenberg.sents('carroll-alice.txt') # returns as a list of lists of strings -_-\n",
    "\n",
    "# Format the text\n",
    "stop_list = stopwords.words('english')\n",
    "term_list = []\n",
    "for idx, terms in enumerate(sentences):\n",
    "    # Remove stop words\n",
    "    terms = [w for w in terms if w not in stop_list]\n",
    "    \n",
    "    # Remove 'CHAPTER'\n",
    "    terms = [w for w in terms if re.search(r'CHAPTER', w) is None]\n",
    "    # Remove non-words and short words\n",
    "    terms = [w for w in terms if re.search(r'[A-Za-z]{3,}', w) is not None]\n",
    "    \n",
    "    # If we haven't emptied the list of terms,\n",
    "    if len(terms) > 0:\n",
    "        # make everything lowercase (reduces total size)\n",
    "        terms = [w.lower() for w in terms]\n",
    "    \n",
    "        # add to the list\n",
    "        term_list.append(terms)\n",
    "\n",
    "display(term_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_ids=2472, len_trans=1673\n"
     ]
    }
   ],
   "source": [
    "# Denormalize\n",
    "trans_list = []\n",
    "item_names = {}\n",
    "item_ids = {}\n",
    "\n",
    "id_counter = 0\n",
    "for terms in term_list:\n",
    "    transaction = []\n",
    "    \n",
    "    for term in terms:\n",
    "        if term not in item_ids:\n",
    "            item_ids[term] = id_counter\n",
    "            item_names[id_counter] = term\n",
    "            id_counter += 1\n",
    "            \n",
    "        transaction += [item_ids[term]]\n",
    "    \n",
    "    trans_list += [transaction]\n",
    "    \n",
    "len_ids, len_trans = len(item_ids), len(trans_list)\n",
    "    \n",
    "items = np.arange(0, len_ids)\n",
    "\n",
    "# Information\n",
    "print(f'len_ids={len_ids}, len_trans={len_trans}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Binarize\n",
    "transactions = np.full((len_trans, len_ids), False, dtype=np.bool)\n",
    "\n",
    "for idx, trans in enumerate(trans_list):\n",
    "    for term in trans:\n",
    "        transactions[idx][term] = True \n",
    "        \n",
    "print(f'{transactions[:10].astype(int)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Weka csv\n",
    "out_file = 'alice.csv'\n",
    "with open('alice.csv', 'w', newline='') as file:\n",
    "    csvwriter = csv.writer(file, delimiter=',', quoting=csv.QUOTE_ALL, quotechar=\"'\", lineterminator='\\n')\n",
    "    csvwriter.writerow([item_names[i] for i in range(len_ids)])\n",
    "    \n",
    "    for idx in range(len_trans):\n",
    "        csvwriter.writerow(list(map(lambda x: '' if not x else 'True', transactions[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) MNIST NeuralNetworkMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier with 2 hidden layers had accuracies 0.900, 0.994, 0.906, 0.956, 0.972, 0.961, 0.978, 0.961, 0.939, 0.961 in\n",
      "10-fold cross-validation.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "digits = load_digits()['images']\n",
    "digits = digits.reshape((len(digits), -1))\n",
    "labels = load_digits()['target']\n",
    "\n",
    "# Make NN\n",
    "hidden_layers = (64, 64)\n",
    "NN = nn(hidden_layers)\n",
    "\n",
    "# Cross-validate\n",
    "kfold = KFold(n_splits=10)\n",
    "scores = cross_val_score(NN, digits, labels, cv=kfold, scoring='accuracy', n_jobs=4)\n",
    "scores_str = ''\n",
    "for idx, score in enumerate(scores):\n",
    "    this_score = f'{score:.3f}'\n",
    "    \n",
    "    scores_str += this_score\n",
    "    \n",
    "    if idx < len(scores) - 1:\n",
    "        scores_str += ', '\n",
    "    \n",
    "print(f'MLPClassifier with 2 hidden layers had accuracies {scores_str} in\\n10-fold cross-validation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
