{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compare and Contrast Classifiers \n",
    "\n",
    "### *Preceptrons*\n",
    "Preceptrons are a rudimentary implementation of artificial neurons with a learning rule for the automatic learning of weight coefficients.\n",
    "\n",
    "* Function - Used for classification. Creates linear separations in data through the summation of datapoints and subsequent adjustment of coefficient multipliers to the datapoints.\n",
    "\n",
    "* Data type - Can only be used if the data is linearally separable, but can be used with any datatype in which that is possible (text, images, or numerical).\n",
    "\n",
    "* Best Use Case - (?) With the proliferation of machine learning techniques I don't think these are ever really useful in practice, but they are a good introduction to ANNs.\n",
    " \n",
    "    \n",
    "### *SVMs*\n",
    "\n",
    "SVMs create hyperplanes to classify data in to appropriate bins. \n",
    "\n",
    "* Function - Used for classification. Optimizes the placement of hyperplanes to accurately separate classes of input.\n",
    "\n",
    "* Data type - SVMs are very good at linearly separable data, such as images (MNIST), text (sentiment), and even protein classification.\n",
    "\n",
    "* Best Use Case - Work best when the data has good distance between the data points, however interpretations exist that allow for \"bendy\" hyperplanes to work around closely related data.\n",
    "\n",
    "### *Decision Trees*\n",
    "\n",
    "Decision Trees area supervised learning model that attempts to create binary trees and filter the data into categories with each branch representing a decision based on a single feature (like flow charts). \n",
    "\n",
    "* Function - Good for classification and regression. Breaks the data into smaller and smaller branches of classes or probabilities.\n",
    "\n",
    "* Data type - For use with multi-dimensional data, but could be used with uni-dimensional data. Works well with probabilitic data such as in predicting the most probable best response in chatbots. Can also be used with image data.\n",
    "\n",
    "* Best Use Case - Works best when all the data features easily fall into easily (non-lazy human) separable categories. Decisions Trees often overfit, so we need to be aware of that possibility (can be solved through the use of Random Forests).\n",
    "\n",
    "### *Random Forests*\n",
    "\n",
    "Random Forest algorithms create a collection of Decision Trees which produce classifications collectively (the classification with the most \"votes\" wins).\n",
    "\n",
    "* Function - Classification and regression. The classification can be thought of as the mode of the classifications produced by the ensemble, and regression can be the mean of the probabilites produced by the ensemble.\n",
    "\n",
    "* Data type - Same as Decision Trees.\n",
    "\n",
    "* Best Use Case - Random forests are good when we observe overfitting from a singular Decision Tree, so long as we take steps ensure diversity amoung the ensemble (which can be accomplished by simply dividing the training set differently for each of the trees).\n",
    "\n",
    "__References:__\n",
    "\n",
    "[1] Wikipedia for each\n",
    "\n",
    "[2] Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning : Machine Learning and Deep Learning with Python, Scikit-learn, and Tensorflow 2, 3rd Edition. 3rd ed. Birmingham: Packt Publishing, Limited, 2019.\n",
    "\n",
    "[3] https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n",
    "\n",
    "[4] https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14\n",
    "\n",
    "[5] https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Feature Types\n",
    "\n",
    "* *Numerical* - Numerical data can be represented as any numerical datatype like integers, floats, or doubles.The Iris dataset from scikit-learn/UCI (https://archive.ics.uci.edu/ml/datasets/iris) features 4 float-type numerical features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "\n",
    "* *Nominal* - Nominal data can be represented as strings or strings can be mapped to integers to reduce storage size. Nominal data is used to categorize data, as in the Iris dataset above which features a 'class' feature that is a integer which mapped to the string for the species of Iris flower.\n",
    "\n",
    "\n",
    "* *Date* - Dates may be stored as either numericals (1982.300 for the the 300th day in 1982) or as strings (\"9-10-2020\"). I found a dataset of international football results that features the date of the match as a string here: https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017\n",
    "\n",
    "\n",
    "* *Text* - Text data may be stored as strings if the text is short or non-repeating (for example name data), but this is usually not particularly useful if the text is large such as in a TV script for The Simpsons. Frequently it is neccessary to distribute text data as a text file but then preprocess that into a list of individual words and/or punctuations. Words may then be mapped to integers in order to reduce total memory usage if necessary (frequently useful). Example of text data can be found in the football results dataset mentioned above which records the home team, away team, and match location as strings. Another example for longer text would be say a collection of text files for the Harry Potter books.\n",
    "\n",
    "\n",
    "* *Image* - Image data should be represented as multidimensional arrays of numericals. The dimensions of the text would correspond to different RGB values necessary to represent the pixel of the image. If the image is black-and-white we can just use a unidimentional array to represent the intensity of each pixel of the image.\n",
    "\n",
    "\n",
    "* *Dependent Variable* - The dependent variable or target may be any of the above feature types. Any dataset with labeled categories (supervised learning datasets) will feature this (\"Class\" in the Iris dataset or \"Chance to Admit\" in the Graduate Admissions dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accuracy Metrics\n",
    "\n",
    "\n",
    "### Confusion Matrix Metrics\n",
    "There are four definable metrics other than accuracy that come from the confusion matrix.\n",
    "* _Precision_ - Precision is the ratio of true positives to the sum of all positives. This tells us/the algo how many classifications where correctly identified in one category, as opposed to accuracy which tells us how many of all classifications were made to correct categories. You could also due the exact opposite of this with the negatives.\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "* _Recall or Sensitivity_ - Recall is the measure of true positives to all positives (true positives + false negatives). This metric tells us the proportion of hits to possible hits or how well the model identifies positives.\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "* _Specificity_ - Specificity is the exact opposite of Sensitivity. That is to say it is the measure of true negatives to  all negatives, which tells us how correctly the model identifies negatives.\n",
    "\n",
    "$$\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}}$$\n",
    "\n",
    "* _F1 Score_ - The F1 Score is a combination of Precision and Recall using the Harmonic Mean of the two. This prevents a false sense of accomplishment when the model has a high recall but low precision (good true positives w/ bad false positives).\n",
    "\n",
    "$$\\text{F1} = \\frac{2 * \\text{Recall} * \\text{Sensitivity}}{\\text{Recall} + \\text{Sensitivity}}$$\n",
    "\n",
    "### Other Metrics\n",
    "\n",
    "These metrics tend to work with values produced by a model rather than the absolute correctness of the model (no confusion matrix).\n",
    "\n",
    "* _Logarithmic Loss_ - Log Loss penalizes false classifications and is especially useful for classifications on multiple classes (that is not just positive or negative, as in the Confusion Matrix Metrics). This metric takes the negative log of the likelihood that the model predicts the outcome that is observed in the data with a lower log loss leading to greater chance the model predicts the outcome correctly.\n",
    "\n",
    "$$\\text{Log Loss} = \\frac{-1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{ij} * log(p_{ij})$$\n",
    "\n",
    "* _Mean Absolute Error_ - Mean Absolute Error is simply the mean of the difference of all observed values to all predicted values. As predicted values get closer to observed values this metric approaches zero. \n",
    "\n",
    "$$\\text{Mean Absolute Error} = \\frac{1}{N} \\sum_{j=1}^{N} |predicted - observed|$$\n",
    "\n",
    "* _Mean Squared Error_ - MSE is similar to Mean Absolute Error, but instead averages the square of the difference between predicted and observed.\n",
    "\n",
    "$$\\text{Mean Squared Error} = \\frac{1}{N} \\sum_{j=1}^{N} (predicted - observed)^2$$\n",
    "\n",
    "\n",
    "__References:__\n",
    "\n",
    "[1] https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n",
    "\n",
    "[2] https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "\n",
    "[3] https://www.kaggle.com/dansbecker/what-is-log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation in Admission Prediction Data\n",
    "\n",
    "I tried using df.equals() to check the equality between the original dataframes corr() and the correlation i made, but that seems to not work despite the dataframes looking the same. My guess is this is some rounding difference. \n",
    "\n",
    "The code cell will first output my correlation dataframe and then pandas's correlation dataframe for easy comparison (it's 1:1).\n",
    "\n",
    "### Questions\n",
    "* The diagonals of the matrix is all one's because is it is calculating the correlation of a feature to itself (which it must correlate to exactly as the values are the same). \n",
    "\n",
    "* I found it interesting to note that research has very low correlation with the rest of the values as it seems to me a student with research experience would be more likely to want to get into grad. school, and would thus score better on better correlating metrics such as the GRE (though I could see research lowering CGPA which would may cause an issue for admissions).\n",
    "\n",
    "* Based on the information available CGPA is the greatest predictor of admission to grad school followed by GRE. This seems valid as a student who applies themselves well before grad. school is probably more likely to apply themselves well in both grad. school and on the GRE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My correlation:\n",
      "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
      "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
      "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
      "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
      "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
      "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
      "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
      "Research            0.580391     0.489858           0.447783  0.444029   \n",
      "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
      "\n",
      "                       LOR       CGPA  Research  Chance of Admit   \n",
      "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
      "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
      "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
      "SOP                0.729593  0.718144  0.444029          0.675732  \n",
      "LOR                1.000000  0.670211  0.396859          0.669889  \n",
      "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
      "Research           0.396859  0.521654  1.000000          0.553202  \n",
      "Chance of Admit    0.669889  0.873289  0.553202          1.000000  \n",
      "\n",
      "\n",
      "\n",
      "Pandas's correlation:\n",
      "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
      "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
      "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
      "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
      "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
      "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
      "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
      "Research            0.580391     0.489858           0.447783  0.444029   \n",
      "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
      "\n",
      "                       LOR       CGPA  Research  Chance of Admit   \n",
      "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
      "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
      "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
      "SOP                0.729593  0.718144  0.444029          0.675732  \n",
      "LOR                1.000000  0.670211  0.396859          0.669889  \n",
      "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
      "Research           0.396859  0.521654  1.000000          0.553202  \n",
      "Chance of Admit    0.669889  0.873289  0.553202          1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "# path to downloaded data\n",
    "data_path = './datasets/Admission_Predict.csv'\n",
    "\n",
    "# initalize df for holding admission data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# if the data_path is wrong\n",
    "if not isfile(data_path):\n",
    "    print(\"Please make sure the data_path is correct and that the data is\" + \\\n",
    "          \"named appropriately\")\n",
    "\n",
    "# otherwise intialize the dataframe\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.drop(['Serial No.'], axis=1) # remove Serial No. as it has no correlation\n",
    "\n",
    "# dict for holding calculated correlations\n",
    "corr_dict = {}\n",
    "\n",
    "# for each column pair possible\n",
    "for f1, f2 in it.product(df.columns, repeat=2):\n",
    "    # get the pearson correlation \n",
    "    # produces a 2x2 matrix with the correlation we want at [0][1] and [1][0]\n",
    "    corr = np.corrcoef(df[f1], df[f2])[0][1]\n",
    "    \n",
    "    # if feature 1 already in dict, just append corr\n",
    "    # else add feature 1 and apply corr as a list object\n",
    "    if f1 in corr_dict:\n",
    "        corr_dict[f1].append(corr)\n",
    "    else:\n",
    "        corr_dict[f1] = [corr]\n",
    "\n",
    "# make a dataframe from the correlation dictionary\n",
    "corr_df = pd.DataFrame(corr_dict)\n",
    "\n",
    "# get the names of the columns as a dict\n",
    "col_dict = {}\n",
    "for idx, col in enumerate(corr_df.columns):\n",
    "    col_dict[idx] = col\n",
    "\n",
    "# rename the columns of the corr_df\n",
    "corr_df = corr_df.rename(index=col_dict)\n",
    "\n",
    "# output my results and pandas corr()\n",
    "print(f\"My correlation:\\n{corr_df}\")\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Pandas's correlation:\\n{df.corr()}\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
