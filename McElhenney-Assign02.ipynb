{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compare and Contrast Classifiers \n",
    "\n",
    "* *Preceptrons*\n",
    "    A preceptron is a rudimentary artificial neuron with an associated learning rule to allow it to automatically find optimal weights for the learning coefficients. This allows for the categorization of input data based on binary output.\n",
    "* *SVMs*\n",
    "* *Decision Trees*\n",
    "* *Random Forests*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Feature Types\n",
    "\n",
    "* *Numerical* - Numerical data can be represented as any numerical datatype like integers, floats, or doubles.The Iris dataset from scikit-learn/UCI (https://archive.ics.uci.edu/ml/datasets/iris) features 4 float-type numerical features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "\n",
    "* *Nominal* - Nominal data can be represented as strings or strings can be mapped to integers to reduce storage size. Nominal data is used to categorize data, as in the Iris dataset above which features a 'class' feature that is a integer which mapped to the string for the species of Iris flower.\n",
    "\n",
    "\n",
    "* *Date* - Dates may be stored as either numericals (1982.300 for the the 300th day in 1982) or as strings (\"9-10-2020\"). I found a dataset of international football results that features the date of the match as a string here: https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017\n",
    "\n",
    "\n",
    "* *Text* - Text data may be stored as strings if the text is short or non-repeating (for example name data), but this is usually not particularly useful if the text is large such as in a TV script for The Simpsons. Frequently it is neccessary to distribute text data as a text file but then preprocess that into a list of individual words and/or punctuations. Words may then be mapped to integers in order to reduce total memory usage if necessary (frequently useful). Example of text data can be found in the football results dataset mentioned above which records the home team, away team, and match location as strings. Another example for longer text would be say a collection of text files for the Harry Potter books.\n",
    "\n",
    "\n",
    "* *Image* - Image data should be represented as multidimensional arrays of numericals. The dimensions of the text would correspond to different RGB values necessary to represent the pixel of the image. If the image is black-and-white we can just use a unidimentional array to represent the intensity of each pixel of the image.\n",
    "\n",
    "\n",
    "* *Dependent Variable* - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Accuracy Metrics\n",
    "\n",
    "\n",
    "#### Confusion Matrix Metrics\n",
    "There are four definable metrics other than accuracy that come from the confusion matrix.\n",
    "* _Precision_ - Precision is the ratio of true positives to the sum of all positives. This tells us/the algo how many classifications where correctly identified in one category, as opposed to accuracy which tells us how many of all classifications were made to correct categories. You could also due the exact opposite of this with the negatives.\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
    "\n",
    "* _Recall or Sensitivity_ - Recall is the measure of true positives to all positives (true positives + false negatives). This metric tells us the proportion of hits to possible hits or how well the model identifies positives.\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
    "\n",
    "* _Specificity_ - Specificity is the exact opposite of Sensitivity. That is to say it is the measure of true negatives to  all negatives, which tells us how correctly the model identifies negatives.\n",
    "\n",
    "$$\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}}$$\n",
    "\n",
    "* _F1 Score_ - The F1 Score is a combination of Precision and Recall using the Harmonic Mean of the two. This prevents a false sense of accomplishment when the model has a high recall but low precision (good true positives w/ bad false positives).\n",
    "\n",
    "$$\\text{F1} = \\frac{2 * \\text{Recall} * \\text{Sensitivity}}{\\text{Recall} + \\text{Sensitivity}}$$\n",
    "\n",
    "#### Other Metrics\n",
    "\n",
    "These metrics tend to work with values produced by a model rather than the absolute correctness of the model (no confusion matrix).\n",
    "\n",
    "* _Logarithmic Loss_ - Log Loss penalizes false classifications and is especially useful for classifications on multiple classes (that is not just positive or negative, as in the Confusion Matrix Metrics). This metric takes the negative log of the likelihood that the model predicts the outcome that is observed in the data with a lower log loss leading to greater chance the model predicts the outcome correctly.\n",
    "\n",
    "$$\\text{Log Loss} = \\frac{-1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{ij} * log(p_{ij})$$\n",
    "\n",
    "* _Mean Absolute Error_ - Mean Absolute Error is simply the mean of the difference of all observed values to all predicted values. As predicted values get closer to observed values this metric approaches zero. \n",
    "\n",
    "$$\\text{Mean Absolute Error} = \\frac{1}{N} \\sum_{j=1}^{N} |predicted - observed|$$\n",
    "\n",
    "* _Mean Squared Error_ - MSE is similar to Mean Absolute Error, but instead averages the square of the difference between predicted and observed.\n",
    "\n",
    "$$\\text{Mean Squared Error} = \\frac{1}{N} \\sum_{j=1}^{N} (predicted - observed)^2$$\n",
    "\n",
    "\n",
    "__References:__\n",
    "\n",
    "[1] https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\n",
    "\n",
    "[2] https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "\n",
    "[3] https://www.kaggle.com/dansbecker/what-is-log-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Correlation in Admission Prediction Data\n",
    "\n",
    "I tried using df.equals() to check the equality between the original dataframes corr() and the correlation i made, but that seems to not work despite the dataframes looking the same. My guess is this is some rounding difference. \n",
    "\n",
    "This cell will first output my correlation dataframe and then pandas's correlation dataframe for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My correlation:\n",
      "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
      "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
      "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
      "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
      "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
      "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
      "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
      "Research            0.580391     0.489858           0.447783  0.444029   \n",
      "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
      "\n",
      "                       LOR       CGPA  Research  Chance of Admit   \n",
      "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
      "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
      "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
      "SOP                0.729593  0.718144  0.444029          0.675732  \n",
      "LOR                1.000000  0.670211  0.396859          0.669889  \n",
      "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
      "Research           0.396859  0.521654  1.000000          0.553202  \n",
      "Chance of Admit    0.669889  0.873289  0.553202          1.000000  \n",
      "\n",
      "\n",
      "\n",
      "Pandas's correlation:\n",
      "                   GRE Score  TOEFL Score  University Rating       SOP  \\\n",
      "GRE Score           1.000000     0.835977           0.668976  0.612831   \n",
      "TOEFL Score         0.835977     1.000000           0.695590  0.657981   \n",
      "University Rating   0.668976     0.695590           1.000000  0.734523   \n",
      "SOP                 0.612831     0.657981           0.734523  1.000000   \n",
      "LOR                 0.557555     0.567721           0.660123  0.729593   \n",
      "CGPA                0.833060     0.828417           0.746479  0.718144   \n",
      "Research            0.580391     0.489858           0.447783  0.444029   \n",
      "Chance of Admit     0.802610     0.791594           0.711250  0.675732   \n",
      "\n",
      "                       LOR       CGPA  Research  Chance of Admit   \n",
      "GRE Score          0.557555  0.833060  0.580391          0.802610  \n",
      "TOEFL Score        0.567721  0.828417  0.489858          0.791594  \n",
      "University Rating  0.660123  0.746479  0.447783          0.711250  \n",
      "SOP                0.729593  0.718144  0.444029          0.675732  \n",
      "LOR                1.000000  0.670211  0.396859          0.669889  \n",
      "CGPA               0.670211  1.000000  0.521654          0.873289  \n",
      "Research           0.396859  0.521654  1.000000          0.553202  \n",
      "Chance of Admit    0.669889  0.873289  0.553202          1.000000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "# path to downloaded data\n",
    "data_path = './datasets/Admission_Predict.csv'\n",
    "\n",
    "# initalize df for holding admission data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# if the data_path is wrong\n",
    "if not isfile(data_path):\n",
    "    print(\"Please make sure the data_path is correct and that the data is\" + \\\n",
    "          \"named appropriately\")\n",
    "\n",
    "# otherwise intialize the dataframe\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.drop(['Serial No.'], axis=1) # remove Serial No. as it has no correlation\n",
    "\n",
    "# dict for holding calculated correlations\n",
    "corr_dict = {}\n",
    "\n",
    "# for each column pair possible\n",
    "for f1, f2 in it.product(df.columns, repeat=2):\n",
    "    # get the pearson correlation \n",
    "    # produces a 2x2 matrix with the correlation we want at [0][1] and [1][0]\n",
    "    corr = np.corrcoef(df[f1], df[f2])[0][1]\n",
    "    \n",
    "    # if feature 1 already in dict, just append corr\n",
    "    # else add feature 1 and apply corr as a list object\n",
    "    if f1 in corr_dict:\n",
    "        corr_dict[f1].append(corr)\n",
    "    else:\n",
    "        corr_dict[f1] = [corr]\n",
    "\n",
    "# make a dataframe from the correlation dictionary\n",
    "corr_df = pd.DataFrame(corr_dict)\n",
    "\n",
    "# get the names of the columns as a dict\n",
    "col_dict = {}\n",
    "for idx, col in enumerate(corr_df.columns):\n",
    "    col_dict[idx] = col\n",
    "\n",
    "# rename the columns of the corr_df\n",
    "corr_df = corr_df.rename(index=col_dict)\n",
    "\n",
    "# output my results and pandas corr()\n",
    "print(f\"My correlation:\\n{corr_df}\")\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Pandas's correlation:\\n{df.corr()}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
